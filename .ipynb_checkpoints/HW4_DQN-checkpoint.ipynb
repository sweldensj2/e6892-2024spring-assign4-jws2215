{
<<<<<<< HEAD
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dCn5gK-8Qhtz"
      },
      "source": [
        "#  Instruction\n",
        "\n",
        "In this notebook, we will learn how to implement DQN using Tensorflow for the [Cartpole environment in OpenAI gym](https://gymnasium.farama.org/environments/classic_control/cart_pole/). You are given a basic skeleton but you need to complete the code where appropriate to solve the cartpole problem.\n",
        "\n",
        "You are free to tweak the code at any part. Your are also free to tweak the hyper-parameters to improve the performance of the agent. At the end you have to evaluate the performance of the agent on 100 independent episodes on the environment and print out the average testing performance.\n",
        "\n",
        "Make sure that your final submission is a notebook that can be run from beginning to end, and that you print out the performance of the agent at the end of the notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7-Vo-FNPRX9V"
      },
      "source": [
        "import os\n",
        "#!{os.sys.executable} -m pip install gymnasium\n",
        "#!{os.sys.executable} -m pip install Pillow\n",
        "#!{os.sys.executable} -m pip install ipython\n",
        "#!{os.sys.executable} -m pip install pygame\n",
        "\n",
        "from PIL import Image\n",
        "from IPython.display import display\n",
        "\n",
        "import tensorflow as tf\n",
        "from collections import deque\n",
        "import numpy as np\n",
        "%matplotlib inline\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import gym\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hZNKbkOPTeLs"
      },
      "source": [
        "# Parameters\n",
        "gamma = 0.99  # discount \n",
        "envname = \"CartPole-v1\"  # environment name\n",
        "env=gym.make(envname, render_mode=\"rgb_array\")\n",
        "\n",
        "obssize = env.observation_space.low.size\n",
        "actsize = env.action_space.n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GujIeXW2RulT"
      },
      "source": [
        "## DQN (Deep Q Network)\n",
        "\n",
        "In previous HWs, we have learned to use Tensorflow to build deep learning models. In this HW, we will apply deep learning as function approximations in reinforcement learning. \n",
        "\n",
        "Reference: DQN https://arxiv.org/abs/1312.5602\n",
        "\n",
        "\n",
        "In tabular Q-learning, we maintain a table of state-action pairs $(s,a)$ and save one action value for each entry $Q(s,a),\\forall (s,a)$. At each time step $t$, we are in state $s_t$, then we choose action based on $\\epsilon-$greedy strategy. With prob $\\epsilon$, choose action uniformly random; with prob $1-\\epsilon$, choose action based on $$a_t = \\arg\\max_a Q(s_t,a)$$ \n",
        "\n",
        "We then get the instant reward $r_t$, update the Q-table using the following rule\n",
        "\n",
        "$$Q(s_t,a_t) \\leftarrow (1-\\alpha)Q(s_t,a_t) + \\alpha (r_t + \\max_a \\gamma Q(s_{t+1},a))$$\n",
        "\n",
        "where $\\alpha \\in (0,1)$ is learning rate. The algorithm is shown to converge in tabular cases. However, in cases where we cannot keep a table for state and action, we need function approximation. Consider using neural network with parameter $\\theta$, the network takes as input state $s$ and action $a$. (*or some features of state and action*). Let $Q_\\theta(s,a)$ be the output of the network, which estimates the optimal Q-value function for state $s$ and action $a$.\n",
        "$$Q_\\theta(s,a) \\approx Q^\\ast(s,a)$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "494xoDa8SLHG"
      },
      "source": [
        "def model_creator():\n",
        "    model = keras.Sequential()\n",
        "    model.add(layers.Dense(5,activation=\"relu\"))\n",
        "    # you should later modify this neural network\n",
        "    model.add(layers.Dense(actsize,activation=\"linear\")) # you should have one output for each possible action\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9BlBZLeSh2f"
      },
      "source": [
        "We wish to train the neural network in order to find $\\theta$ such that $Q_\\theta(s,a)$ approximates $Q^*(s,a)$. As we discussed in the class, we can use observations of form $(s_i, a_i, r_i, s'_{i})$ (i.e., observing reward $r_i$ and new state $s'_{i}$ on taking action $a_i$ in state $s_i$) for training. Based on observations, we can use stochastic gradient descent to update $\\theta$ in the direction that minimizes the loss function. Further, based on values $Q_\\theta(s,a)$, we can choose the action based on $\\epsilon$-greedy policy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bHaI6w4BSc3Q"
      },
      "source": [
        "Formally let $d_i$ be the target for the $i$-th sample $(s_t,a_t,r_t,s_{t+1})$\n",
        "\n",
        "$$d_i =  r_t +   \\gamma \\max_a Q_\\theta(s_{t+1},a)$$\n",
        "\n",
        "We can collect a batch of $N$ samples (this generalizes the per sample update $N=1$ discusssed in class), consider the loss fucntion,\n",
        "\n",
        "$$J:=\\frac{1}{N} \\sum_{i=1}^N (Q_\\theta(s_i,a_i) - d_i)^2$$\n",
        "\n",
        "and update\n",
        "\n",
        "$$\n",
        "\\theta \\leftarrow \\theta -\\alpha \\nabla_\\theta J\n",
        "$$\n",
        "\n",
        "This procedure has been shown to be fairly unstable. In class, we discussed two techniques to stabilize the training process: target network and replay buffer.\n",
        "\n",
        "**Replay Buffer**\n",
        "Maintain a buffer $R$ to store trainsition tuples $(s_t,a_t,r_t,s_{t+1})$. When minimizing the loss, we sample batches from the replay buffer and compute gradients for update on these batches. In particular, in each update, we sample $N$ tuples $(s_t,a_t,r_t,s_{t+1})$ from buffer $R$ and then minimize the\n",
        "loss \n",
        "\n",
        "$$\\frac{1}{N} \\sum_{i=1}^N (Q_\\theta(s_i,a_i) -  (r_i + \\gamma \\max_a Q_\\theta(s_i^\\prime,a))^2$$\n",
        "\n",
        "and update parameters.\n",
        "\n",
        "**Target Network**\n",
        "Maintain a target network in addition to the original principal network. The target network is just a copy of the original network but the parameters are not updated by gradients. The target network $\\theta^{-}$ is copied from the principal network every $\\tau$ time steps. Target network is used to compute the targets for update\n",
        "\n",
        "$$d_i = \\max_a r_t + \\gamma Q_{\\theta^{-}}(s_{i}^\\prime,a)$$\n",
        "\n",
        "the targets are used in the loss function to update the principal network parameters. This slowly updated target network ensures that the targets come from a relatively stationary distribution and hence stabilize learning."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WKt2Mt6TTZsk"
      },
      "source": [
        "# batch_size = 32\n",
        "# Model used for selecting actions (principal)\n",
        "model = model_creator()\n",
        "# Then create the target model. This will periodically be copied from the principal network \n",
        "model_target = model_creator()\n",
        "\n",
        "model.build((batch_size,obssize,))\n",
        "model_target.build((batch_size,obssize,))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0JA7a0AXiVO7"
      },
      "source": [
        "- Complete the code below to learn an Agent using DQN. \n",
        "- You should tweak the Neural network appropriately to achieve a good reward (>100). Ideally you would want to have a reward close to 200.\n",
        "- The reference paper performs updates every 4 actions. You can experiment with this parameter to speed up the learning\n",
        "- You can experiment with other parameters as well, like learning rate, memory size, different exploration schemes (e.g., adaptive $\\epsilon$ or strategic explorations with bonus rewards) and others.\n",
        "\n",
        "- As we mentioned in class, there are multiple ways to improve the efficiency even further. OPTIONALLY you can experiment with these:\n",
        "  - Prioritized Replay buffer.\n",
        "  - Double DQN \n",
        "  -Dueling DQN architecture.\n",
        "\n",
        "- In case you need to debug your code you can try printing relevant information as the training happens. For example although the performance might vary from iteration to iteration, the average Q values might increase overtime in a more smooth way. This is discussed in the refernece paper\n",
        "\n",
        "- Create a plot of the running reward sampled throughout the training at the frequency of your choice at the end of the training\n",
        "- OPTIONALLY you can create a plot for the average Q-values of the principal Q-network sampled at the frequency of your choice\n",
        "\n",
        "- Ideally you want to learn with as few episodes as possible. However you will not be graded on sample efficiency in this homework. You encouraged to try to learn efficiently though.\n",
        "\n",
        "- Note that the skeleton code includes the GradientTape construct to do the learning. Take a look [here](https://www.tensorflow.org/api_docs/python/tf/GradientTape) for an explanation of GradientTape. It allows for more flexibility than model.fit. Also it uses Adam (Adaptive Moment Estimation) for Stochastic Gradient Descent optimizer. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kn81y4Iz_QlJ"
      },
      "source": [
        "optimizer = keras.optimizers.Adam(learning_rate=0.0005)\n",
        "\n",
        "# Our Experience Replay memory \n",
        "action_history = []\n",
        "state_history = []\n",
        "state_next_history = []\n",
        "rewards_history = []\n",
        "done_history = []\n",
        "episode_reward_history = []\n",
        "\n",
        "# Replay memory size\n",
        "max_memory = 1000 # You can experiment with different sizes.\n",
        "\n",
        "running_reward = 0\n",
        "episode_count = 0\n",
        "timestep_count = 0\n",
        "\n",
        "\n",
        "# how often to train your model - this allows you to speed up learning\n",
        "# by not performing in every iteration learning. See also refernece paper\n",
        "# you can set this value to other values like 1 as well to learn every time \n",
        "\n",
        "update_after_actions = 4\n",
        "\n",
        "# How often to update the target network\n",
        "# target_update_every = 1000\n",
        "loss_function = keras.losses.Huber() # You can use the Huber loss function or the mean squared error function \n",
        "max_steps_per_episode = 1000\n",
        "\n",
        "# max_episodes = 500\n",
        "# max_steps_per_episode = 1000\n",
        "last_n_reward = 100\n",
        "\n",
        "for episode in range(max_episodes):\n",
        "    state = np.array(env.reset())\n",
        "    episode_reward = 0\n",
        "\n",
        "    for timestep in range(1, max_steps_per_episode):\n",
        "        timestep_count += 1\n",
        "\n",
        "        # exploration\n",
        "        #if ...:\n",
        "            # Take random action\n",
        "            # action = np.random.choice(actsize)\n",
        "        #else:\n",
        "            # Predict action Q-values\n",
        "            # From environment state\n",
        "            # state_t = tf.convert_to_tensor(state)\n",
        "            # state_t = tf.expand_dims(state_tensor, 0)\n",
        "            # action_vals = model(state_t, training=False)\n",
        "            # Choose the best action\n",
        "            # action = ....\n",
        "\n",
        "        # follow selected action\n",
        "        state_next, reward, done, _ = env.step(action)\n",
        "        state_next = np.array(state_next)\n",
        "        episode_reward += reward\n",
        "\n",
        "        # Save action/states and other information in replay buffer\n",
        "        # action_history.append(action)\n",
        "        # ...\n",
        "        # ...\n",
        "\n",
        "        state = state_next\n",
        "\n",
        "        # Update every Xth frame to speed up (optional)\n",
        "        # and if you have sufficient history\n",
        "        if timestep_count % update_after_actions == 0 and len(action_history) > batch_size:\n",
        "\n",
        "            #  Sample a set of batch_size memories from the history\n",
        "\n",
        "            # state_sample = ...\n",
        "            # state_next_sample = ...\n",
        "            # rewards_sample = ...\n",
        "            # action_sample = ...\n",
        "            # done_sample = ...\n",
        "\n",
        "            # Create for the sample states the targets (r+gamma * max Q(...) )\n",
        "            # Q_next_state = model_target.predict(...)\n",
        "            # Q_targets = rewards_sample + gamma * ...\n",
        "\n",
        "            # If the episode was ended (done_sample value is 1)\n",
        "            # you can penalize the Q value of the target by some value `penalty`\n",
        "            # Q_targets = Q_targets * (1 - done_sample) - penalty*done_sample\n",
        "\n",
        "\n",
        "            # What actions are relevant and need updating\n",
        "            relevant_actions = tf.one_hot(action_sample, actsize)\n",
        "            # we will use Gradient tape to do a custom gradient \n",
        "            # in the `with` environment we will record a set of operations\n",
        "            # and then we will take gradients with respect to the trainable parameters\n",
        "            # in the neural network\n",
        "            with tf.GradientTape() as tape:\n",
        "                # Train the model on your action selecting network\n",
        "                #q_values = model(state_sample) \n",
        "                # We consider only the relevant actions\n",
        "                #Q_of_actions = tf.reduce_sum(tf.multiply(q_values, relevant_actions), axis=1)\n",
        "                # Calculate loss between principal network and target network\n",
        "                #loss = loss_function(Q_targets, Q_of_actions)\n",
        "\n",
        "            # Nudge the weights of the trainable variables towards \n",
        "            # grads = tape.gradient(loss, model.trainable_variables)\n",
        "            # optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "        if timestep_count % target_update_every == 0:\n",
        "            # update the the target network with new weights\n",
        "            model_target.set_weights(model.get_weights())\n",
        "            # Log details\n",
        "            template = \"running reward: {:.2f} at episode {}, frame count {}, epsilon {}\"\n",
        "            print(template.format(running_reward, episode_count, frame_count,epsilon))\n",
        "\n",
        "        # Don't let the memory grow beyond the limit\n",
        "        if len(rewards_history) > max_memory:\n",
        "            del rewards_history[:1]\n",
        "            del state_history[:1]\n",
        "            del state_next_history[:1]\n",
        "            del action_history[:1]\n",
        "            del done_history[:1]\n",
        "        if done: break\n",
        "\n",
        "    # reward of last 100\n",
        "    episode_reward_history.append(episode_reward)\n",
        "    if len(episode_reward_history) > last_n_reward: del episode_reward_history[:1]\n",
        "    running_reward = np.mean(episode_reward_history)\n",
        "    episode_count += 1\n",
        "\n",
        "    # If you want to stop your training once you achieve the reward you want you can\n",
        "    # have an if statement here. Alternatively you can stop after a fixed number\n",
        "    # of episodes."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y-kUmOPTiMXX"
      },
      "source": [
        "Evaluate the performance of the agent on 100 episodes on the environment and print out the average testing performance. Alternatively you can make sure the code above terminates with 100 episodes where there is no exploration at all (epsilon=0)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xe2V09jdiNXI"
      },
      "source": [
        "# YOUR CODE Here"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
=======
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dCn5gK-8Qhtz"
   },
   "source": [
    "#  Instruction\n",
    "\n",
    "In this notebook, we will learn how to implement DQN using Tensorflow for the [Cartpole environment in OpenAI gym](https://gymnasium.farama.org/environments/classic_control/cart_pole/). You are given a basic skeleton but you need to complete the code where appropriate to solve the cartpole problem.\n",
    "\n",
    "You are free to tweak the code at any part. Your are also free to tweak the hyper-parameters to improve the performance of the agent. At the end you have to evaluate the performance of the agent on 100 independent episodes on the environment and print out the average testing performance.\n",
    "\n",
    "Make sure that your final submission is a notebook that can be run from beginning to end, and that you print out the performance of the agent at the end of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "7-Vo-FNPRX9V",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-10 19:14:54.022233: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# !{os.sys.executable} -m pip install gymnasium\n",
    "# !{os.sys.executable} -m pip install Pillow\n",
    "# !{os.sys.executable} -m pip install ipython\n",
    "# !{os.sys.executable} -m pip install pygame\n",
    "\n",
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "\n",
    "import tensorflow as tf\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import gymnasium as gym\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "hZNKbkOPTeLs",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "gamma = 0.99  # discount \n",
    "envname = \"CartPole-v1\"  # environment name\n",
    "env=gym.make(envname, render_mode=\"rgb_array\")\n",
    "\n",
    "obssize = env.observation_space.low.size\n",
    "actsize = env.action_space.n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GujIeXW2RulT"
   },
   "source": [
    "## DQN (Deep Q Network)\n",
    "\n",
    "In previous HWs, we have learned to use Tensorflow to build deep learning models. In this HW, we will apply deep learning as function approximations in reinforcement learning. \n",
    "\n",
    "Reference: DQN https://arxiv.org/abs/1312.5602\n",
    "\n",
    "\n",
    "In tabular Q-learning, we maintain a table of state-action pairs $(s,a)$ and save one action value for each entry $Q(s,a),\\forall (s,a)$. At each time step $t$, we are in state $s_t$, then we choose action based on $\\epsilon-$greedy strategy. With prob $\\epsilon$, choose action uniformly random; with prob $1-\\epsilon$, choose action based on $$a_t = \\arg\\max_a Q(s_t,a)$$ \n",
    "\n",
    "We then get the instant reward $r_t$, update the Q-table using the following rule\n",
    "\n",
    "$$Q(s_t,a_t) \\leftarrow (1-\\alpha)Q(s_t,a_t) + \\alpha (r_t + \\max_a \\gamma Q(s_{t+1},a))$$\n",
    "\n",
    "where $\\alpha \\in (0,1)$ is learning rate. The algorithm is shown to converge in tabular cases. However, in cases where we cannot keep a table for state and action, we need function approximation. Consider using neural network with parameter $\\theta$, the network takes as input state $s$ and action $a$. (*or some features of state and action*). Let $Q_\\theta(s,a)$ be the output of the network, which estimates the optimal Q-value function for state $s$ and action $a$.\n",
    "$$Q_\\theta(s,a) \\approx Q^\\ast(s,a)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "494xoDa8SLHG",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def model_creator():\n",
    "    model = keras.Sequential()\n",
    "    model.add(layers.Dense(5,activation=\"relu\"))\n",
    "    # you should later modify this neural network\n",
    "    model.add(layers.Dense(actsize,activation=\"linear\")) # you should have one output for each possible action\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a9BlBZLeSh2f"
   },
   "source": [
    "We wish to train the neural network in order to find $\\theta$ such that $Q_\\theta(s,a)$ approximates $Q^*(s,a)$. As we discussed in the class, we can use observations of form $(s_i, a_i, r_i, s'_{i})$ (i.e., observing reward $r_i$ and new state $s'_{i}$ on taking action $a_i$ in state $s_i$) for training. Based on observations, we can use stochastic gradient descent to update $\\theta$ in the direction that minimizes the loss function. Further, based on values $Q_\\theta(s,a)$, we can choose the action based on $\\epsilon$-greedy policy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bHaI6w4BSc3Q"
   },
   "source": [
    "Formally let $d_i$ be the target for the $i$-th sample $(s_t,a_t,r_t,s_{t+1})$\n",
    "\n",
    "$$d_i =  r_t +   \\gamma \\max_a Q_\\theta(s_{t+1},a)$$\n",
    "\n",
    "We can collect a batch of $N$ samples (this generalizes the per sample update $N=1$ discusssed in class), consider the loss fucntion,\n",
    "\n",
    "$$J:=\\frac{1}{N} \\sum_{i=1}^N (Q_\\theta(s_i,a_i) - d_i)^2$$\n",
    "\n",
    "and update\n",
    "\n",
    "$$\n",
    "\\theta \\leftarrow \\theta -\\alpha \\nabla_\\theta J\n",
    "$$\n",
    "\n",
    "This procedure has been shown to be fairly unstable. In class, we discussed two techniques to stabilize the training process: target network and replay buffer.\n",
    "\n",
    "**Replay Buffer**\n",
    "Maintain a buffer $R$ to store trainsition tuples $(s_t,a_t,r_t,s_{t+1})$. When minimizing the loss, we sample batches from the replay buffer and compute gradients for update on these batches. In particular, in each update, we sample $N$ tuples $(s_t,a_t,r_t,s_{t+1})$ from buffer $R$ and then minimize the\n",
    "loss \n",
    "\n",
    "$$\\frac{1}{N} \\sum_{i=1}^N (Q_\\theta(s_i,a_i) -  (r_i + \\gamma \\max_a Q_\\theta(s_i^\\prime,a))^2$$\n",
    "\n",
    "and update parameters.\n",
    "\n",
    "**Target Network**\n",
    "Maintain a target network in addition to the original principal network. The target network is just a copy of the original network but the parameters are not updated by gradients. The target network $\\theta^{-}$ is copied from the principal network every $\\tau$ time steps. Target network is used to compute the targets for update\n",
    "\n",
    "$$d_i = \\max_a r_t + \\gamma Q_{\\theta^{-}}(s_{i}^\\prime,a)$$\n",
    "\n",
    "the targets are used in the loss function to update the principal network parameters. This slowly updated target network ensures that the targets come from a relatively stationary distribution and hence stabilize learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "WKt2Mt6TTZsk",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-10 19:20:32.856709: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-04-10 19:20:33.232182: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-04-10 19:20:33.236646: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-04-10 19:20:33.242249: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-04-10 19:20:33.246022: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-04-10 19:20:33.249560: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-04-10 19:20:33.596656: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-04-10 19:20:33.599105: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-04-10 19:20:33.601426: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-04-10 19:20:33.604753: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1928] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13774 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "# Model used for selecting actions (principal)\n",
    "model = model_creator()\n",
    "# Then create the target model. This will periodically be copied from the principal network \n",
    "model_target = model_creator()\n",
    "\n",
    "model.build((batch_size,obssize,))\n",
    "model_target.build((batch_size,obssize,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0JA7a0AXiVO7"
   },
   "source": [
    "- Complete the code below to learn an Agent using DQN. \n",
    "- You should tweak the Neural network appropriately to achieve a good reward (>100). Ideally you would want to have a reward close to 200.\n",
    "- The reference paper performs updates every 4 actions. You can experiment with this parameter to speed up the learning\n",
    "- You can experiment with other parameters as well, like learning rate, memory size, different exploration schemes (e.g., adaptive $\\epsilon$ or strategic explorations with bonus rewards) and others.\n",
    "\n",
    "- As we mentioned in class, there are multiple ways to improve the efficiency even further. OPTIONALLY you can experiment with these:\n",
    "  - Prioritized Replay buffer.\n",
    "  - Double DQN \n",
    "  -Dueling DQN architecture.\n",
    "\n",
    "- In case you need to debug your code you can try printing relevant information as the training happens. For example although the performance might vary from iteration to iteration, the average Q values might increase overtime in a more smooth way. This is discussed in the refernece paper\n",
    "\n",
    "- Create a plot of the running reward sampled throughout the training at the frequency of your choice at the end of the training\n",
    "- OPTIONALLY you can create a plot for the average Q-values of the principal Q-network sampled at the frequency of your choice\n",
    "\n",
    "- Ideally you want to learn with as few episodes as possible. However you will not be graded on sample efficiency in this homework. You encouraged to try to learn efficiently though.\n",
    "\n",
    "- Note that the skeleton code includes the GradientTape construct to do the learning. Take a look [here](https://www.tensorflow.org/api_docs/python/tf/GradientTape) for an explanation of GradientTape. It allows for more flexibility than model.fit. Also it uses Adam (Adaptive Moment Estimation) for Stochastic Gradient Descent optimizer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "Kn81y4Iz_QlJ"
   },
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block after 'with' statement on line 103 (2358117493.py, line 115)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[11], line 115\u001b[0;36m\u001b[0m\n\u001b[0;31m    if timestep_count % target_update_every == 0:\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block after 'with' statement on line 103\n"
     ]
    }
   ],
   "source": [
    "optimizer = keras.optimizers.Adam(learning_rate=0.0005)\n",
    "\n",
    "# Our Experience Replay memory \n",
    "action_history = []\n",
    "state_history = []\n",
    "state_next_history = []\n",
    "rewards_history = []\n",
    "done_history = []\n",
    "episode_reward_history = []\n",
    "\n",
    "# Replay memory size\n",
    "max_memory = 1000 # You can experiment with different sizes.\n",
    "\n",
    "running_reward = 0\n",
    "episode_count = 0\n",
    "timestep_count = 0\n",
    "\n",
    "\n",
    "# how often to train your model - this allows you to speed up learning\n",
    "# by not performing in every iteration learning. See also refernece paper\n",
    "# you can set this value to other values like 1 as well to learn every time \n",
    "\n",
    "update_after_actions = 4\n",
    "\n",
    "# How often to update the target network\n",
    "# target_update_every = 1000\n",
    "loss_function = keras.losses.Huber() # You can use the Huber loss function or the mean squared error function \n",
    "max_steps_per_episode = 1000\n",
    "\n",
    "max_episodes = 500\n",
    "# max_steps_per_episode = 1000\n",
    "last_n_reward = 100\n",
    "\n",
    "\n",
    "epsilon = 1 #means 100% probability of choosing random action\n",
    "epsilon_decay_rate = 1/(max_episodes*0.9) # reduce the dependence on random actions over time\n",
    "rng = np.random.default_rng() # creates a random number generator to be called later\n",
    "\n",
    "for episode in range(max_episodes):\n",
    "    state = np.array(env.reset())\n",
    "    episode_reward = 0\n",
    "\n",
    "    for timestep in range(1, max_steps_per_episode):\n",
    "        timestep_count += 1\n",
    "        \n",
    "        # exploration\n",
    "        if rng.random() < epsilon:\n",
    "            # Take random action\n",
    "            action = np.random.choice(actsize)\n",
    "        else:\n",
    "            # Predict action Q-values\n",
    "            # From environment state\n",
    "            state_t = tf.convert_to_tensor(state)\n",
    "            state_t = tf.expand_dims(state_tensor, 0)\n",
    "            action_vals = model(state_t, training=False)\n",
    "            # Choose the best action\n",
    "            action = np.argmax(action_vals)\n",
    "\n",
    "        # follow selected action\n",
    "        state_next, reward, done, _ = env.step(action)\n",
    "        state_next = np.array(state_next)\n",
    "        episode_reward += reward\n",
    "\n",
    "        # Save action/states and other information in replay buffer\n",
    "        action_history.append(action)\n",
    "        state_history.append(state)\n",
    "        state_next_history.append(state_next)\n",
    "        rewards_history.append(reward)\n",
    "        done_sample.append(done)\n",
    "        # ...\n",
    "        # ...\n",
    "\n",
    "        state = state_next\n",
    "\n",
    "        # Update every Xth frame to speed up (optional)\n",
    "        # and if you have sufficient history\n",
    "        if timestep_count % update_after_actions == 0 and len(action_history) > batch_size:\n",
    "\n",
    "            #  Sample a set of batch_size memories from the history\n",
    "            random_indices = np.random.choice(len(action_history), batch_size, replace=False)\n",
    "            \n",
    "            state_sample = state_history[random_indices]\n",
    "            state_next_sample = state_next_history[random_indices]\n",
    "            rewards_sample = rewards_history[random_indices]\n",
    "            action_sample = action_history[random_indices]\n",
    "            done_sample = done_sample[random_indices]\n",
    "\n",
    "            # Create for the sample states the targets (r+gamma * max Q(...) )\n",
    "            Q_next_state = model_target.predict(state_next_sample)\n",
    "            # Q_targets = rewards_sample + gamma * ...\n",
    "\n",
    "            # If the episode was ended (done_sample value is 1)\n",
    "            # you can penalize the Q value of the target by some value `penalty`\n",
    "            # Q_targets = Q_targets * (1 - done_sample) - penalty*done_sample\n",
    "\n",
    "\n",
    "            # What actions are relevant and need updating\n",
    "            relevant_actions = tf.one_hot(action_sample, actsize)\n",
    "            # we will use Gradient tape to do a custom gradient \n",
    "            # in the `with` environment we will record a set of operations\n",
    "            # and then we will take gradients with respect to the trainable parameters\n",
    "            # in the neural network\n",
    "            with tf.GradientTape() as tape:\n",
    "                # Train the model on your action selecting network\n",
    "                #q_values = model(state_sample) \n",
    "                # We consider only the relevant actions\n",
    "                #Q_of_actions = tf.reduce_sum(tf.multiply(q_values, relevant_actions), axis=1)\n",
    "                # Calculate loss between principal network and target network\n",
    "                #loss = loss_function(Q_targets, Q_of_actions)\n",
    "\n",
    "            # Nudge the weights of the trainable variables towards \n",
    "            # grads = tape.gradient(loss, model.trainable_variables)\n",
    "            # optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "        if timestep_count % target_update_every == 0:\n",
    "            # update the the target network with new weights\n",
    "            model_target.set_weights(model.get_weights())\n",
    "            # Log details\n",
    "            template = \"running reward: {:.2f} at episode {}, frame count {}, epsilon {}\"\n",
    "            print(template.format(running_reward, episode_count, frame_count,epsilon))\n",
    "            \n",
    "        # Don't let the memory grow beyond the limit\n",
    "        if len(rewards_history) > max_memory:\n",
    "            del rewards_history[:1]\n",
    "            del state_history[:1]\n",
    "            del state_next_history[:1]\n",
    "            del action_history[:1]\n",
    "            del done_history[:1]\n",
    "        if done: break\n",
    "\n",
    "    # reward of last 100\n",
    "    episode_reward_history.append(episode_reward)\n",
    "    if len(episode_reward_history) > last_n_reward: del episode_reward_history[:1]\n",
    "    running_reward = np.mean(episode_reward_history)\n",
    "    episode_count += 1\n",
    "    \n",
    "    epsilon = max(0, epsilon - epsilon_decay_rate)\n",
    "\n",
    "    # If you want to stop your training once you achieve the reward you want you can\n",
    "    # have an if statement here. Alternatively you can stop after a fixed number\n",
    "    # of episodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y-kUmOPTiMXX"
   },
   "source": [
    "Evaluate the performance of the agent on 100 episodes on the environment and print out the average testing performance. Alternatively you can make sure the code above terminates with 100 episodes where there is no exploration at all (epsilon=0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xe2V09jdiNXI"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE Here"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (Local)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
>>>>>>> 4440d1f17040c869b090708c22741a96ebbadc4f
